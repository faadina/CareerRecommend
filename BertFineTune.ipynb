{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution: Counter({0: 10200, 1: 8310, 2: 6763})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import time\n",
    "\n",
    "# Load your CSV\n",
    "career_data = pd.read_csv(\"dataset/datacleanJobstreet.csv\")\n",
    "\n",
    "# Check class distribution\n",
    "print(\"Original class distribution:\", Counter(career_data['job_level_encoded']))\n",
    "\n",
    "# Feature and label separation\n",
    "X = career_data[\"descriptions\"].astype(str)\n",
    "y = career_data[\"job_level_encoded\"]\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define custom Dataset for BERT\n",
    "class CareerDataset(Dataset):\n",
    "    def __init__(self, descriptions, labels, tokenizer, max_len):\n",
    "        self.descriptions = descriptions.fillna(\"No description available\")\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.descriptions)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        description = str(self.descriptions.iloc[item])  # Ensure it's a string\n",
    "        label = self.labels.iloc[item]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            description,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ORIGINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train and evaluate BERT\n",
    "def run_bert_experiment(X, y, train_size, test_size, tokenizer, max_len=128, batch_size=16, epochs=5):\n",
    "    print(f\"\\nTraining split: {train_size*100:.0f}% | Test split: {test_size*100:.0f}%\")\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_size, test_size=test_size, random_state=42)\n",
    "\n",
    "    # Create datasets and loaders\n",
    "    train_dataset = CareerDataset(X_train, y_train, tokenizer, max_len)\n",
    "    test_dataset = CareerDataset(X_test, y_test, tokenizer, max_len)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Load pre-trained BERT model\n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(y.unique()))\n",
    "\n",
    "    # Optimizer and scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "    # Device setup\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    total_start_time = time.time()  # Start total timer\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()  # Start timer for epoch\n",
    "\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "        epoch_end_time = time.time()  # End timer for epoch\n",
    "        epoch_elapsed = epoch_end_time - epoch_start_time\n",
    "        epoch_elapsed_formatted = time.strftime(\"%H:%M:%S\", time.gmtime(epoch_elapsed))\n",
    "\n",
    "        print(f\"Epoch {epoch+1} completed. Average Loss: {avg_loss:.4f} | Time: {epoch_elapsed_formatted}\")\n",
    "\n",
    "    total_end_time = time.time()  # End total timer\n",
    "    total_elapsed = total_end_time - total_start_time\n",
    "    total_elapsed_formatted = time.strftime(\"%H:%M:%S\", time.gmtime(total_elapsed))\n",
    "    print(f\"\\nTotal training time for {epochs} epochs: {total_elapsed_formatted}\")\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            _, predicted = torch.max(logits, dim=1)\n",
    "\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Evaluation code here ...\n",
    "\n",
    "    print(\"\\n BERT + Fine-tuning Results:\")\n",
    "    print(\" Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "    print(\" Classification Report:\\n\", classification_report(y_true, y_pred))\n",
    "    print(\" Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training split: 70% | Test split: 30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed. Average Loss: 0.6002 | Time: 00:13:37\n",
      "Epoch 2 completed. Average Loss: 0.3307 | Time: 00:13:45\n",
      "Epoch 3 completed. Average Loss: 0.3201 | Time: 00:16:32\n",
      "Epoch 4 completed. Average Loss: 0.3194 | Time: 00:44:36\n",
      "Epoch 5 completed. Average Loss: 0.3176 | Time: 00:11:41\n",
      "\n",
      "Total training time for 5 epochs: 01:40:13\n",
      "\n",
      " BERT + Fine-tuning Results:\n",
      " Accuracy: 0.8631354425879844\n",
      " Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.79      0.88      3334\n",
      "           1       0.81      0.96      0.88      2937\n",
      "           2       0.82      0.83      0.82      2168\n",
      "\n",
      "    accuracy                           0.86      8439\n",
      "   macro avg       0.87      0.86      0.86      8439\n",
      "weighted avg       0.88      0.86      0.86      8439\n",
      "\n",
      " Confusion Matrix:\n",
      " [[2644  356  334]\n",
      " [  30 2832   75]\n",
      " [  33  327 1808]]\n",
      "✅ Model and tokenizer saved.\n"
     ]
    }
   ],
   "source": [
    "# Example: Run for 70:30 split\n",
    "model, tokenizer = run_bert_experiment(X, y, train_size=0.7, test_size=0.3, tokenizer=tokenizer)\n",
    "\n",
    "# Save\n",
    "model.save_pretrained(\"bert_finetuned_model\")\n",
    "tokenizer.save_pretrained(\"bert_finetuned_model\")\n",
    "print(\"✅ Model and tokenizer saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training split: 80% | Test split: 20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed. Average Loss: 0.4997 | Time: 00:13:22\n",
      "Epoch 2 completed. Average Loss: 0.3238 | Time: 00:13:24\n",
      "Epoch 3 completed. Average Loss: 0.3116 | Time: 00:13:22\n",
      "Epoch 4 completed. Average Loss: 0.3094 | Time: 00:13:23\n",
      "Epoch 5 completed. Average Loss: 0.3085 | Time: 00:13:25\n",
      "\n",
      "Total training time for 5 epochs: 01:06:57\n",
      "\n",
      " BERT + Fine-tuning Results:\n",
      " Accuracy: 0.8695343050124422\n",
      " Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.80      0.88      2208\n",
      "           1       0.81      0.97      0.88      1994\n",
      "           2       0.82      0.84      0.83      1424\n",
      "\n",
      "    accuracy                           0.87      5626\n",
      "   macro avg       0.87      0.87      0.87      5626\n",
      "weighted avg       0.88      0.87      0.87      5626\n",
      "\n",
      " Confusion Matrix:\n",
      " [[1756  232  220]\n",
      " [  19 1936   39]\n",
      " [  10  214 1200]]\n",
      "✅ Model and tokenizer saved.\n"
     ]
    }
   ],
   "source": [
    "# Example: Run for 80:20 split\n",
    "model, tokenizer = run_bert_experiment(X, y, train_size=0.8, test_size=0.2, tokenizer=tokenizer)\n",
    "\n",
    "# Save\n",
    "model.save_pretrained(\"bert_finetuned_ori_80/20\")\n",
    "tokenizer.save_pretrained(\"bert_finetuned_ori_80/20\")\n",
    "print(\"✅ Model and tokenizer saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training split: 90% | Test split: 10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed. Average Loss: 0.4856 | Time: 00:30:18\n",
      "Epoch 2 completed. Average Loss: 0.3225 | Time: 00:14:50\n",
      "Epoch 3 completed. Average Loss: 0.3127 | Time: 00:14:32\n",
      "Epoch 4 completed. Average Loss: 0.3118 | Time: 00:14:40\n",
      "Epoch 5 completed. Average Loss: 0.3132 | Time: 00:14:46\n",
      "\n",
      "Total training time for 5 epochs: 01:29:08\n",
      "\n",
      " BERT + Fine-tuning Results:\n",
      " Accuracy: 0.8709562744400995\n",
      " Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.80      0.87      1094\n",
      "           1       0.81      0.97      0.88      1000\n",
      "           2       0.85      0.85      0.85       719\n",
      "\n",
      "    accuracy                           0.87      2813\n",
      "   macro avg       0.88      0.87      0.87      2813\n",
      "weighted avg       0.88      0.87      0.87      2813\n",
      "\n",
      " Confusion Matrix:\n",
      " [[874 127  93]\n",
      " [ 15 968  17]\n",
      " [ 17  94 608]]\n",
      "✅ Model and tokenizer saved.\n"
     ]
    }
   ],
   "source": [
    "# Example: Run for 90:10 split\n",
    "model, tokenizer =run_bert_experiment(X, y, train_size=0.9, test_size=0.1, tokenizer=tokenizer)\n",
    "\n",
    "# Save\n",
    "model.save_pretrained(\"bert_finetuned_ori_90/10\")\n",
    "tokenizer.save_pretrained(\"bert_finetuned_ori_90/10\")\n",
    "print(\"✅ Model and tokenizer saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. OVERSAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "def run_bert_oversampling(X, y, train_size, test_size, tokenizer, max_len=128, batch_size=16, epochs=5):\n",
    "    # Split with stratify to maintain class distribution in splits\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, train_size=train_size, test_size=test_size, random_state=42, stratify=y)\n",
    "\n",
    "    print(\"Original:\", Counter(y))\n",
    "    print(\"Training :\", Counter(y_train))\n",
    "    print(\"Test :\", Counter(y_test))\n",
    "\n",
    "    # Oversample training data\n",
    "    ros = RandomOverSampler(random_state=42)\n",
    "    X_train_array = X_train.values.reshape(-1, 1)\n",
    "    X_train_resampled, y_train_resampled = ros.fit_resample(X_train_array, y_train)\n",
    "\n",
    "    print(\"BEFORE oversampling:\", Counter(y_train))\n",
    "    print(\"AFTER oversampling:\", Counter(y_train_resampled))\n",
    "\n",
    "    # Convert back to pandas Series for your Dataset\n",
    "    X_train_resampled = pd.Series(X_train_resampled.flatten())\n",
    "    y_train_resampled = pd.Series(y_train_resampled)\n",
    "\n",
    "    # Create Dataset and DataLoader for training and testing\n",
    "    train_dataset = CareerDataset(X_train_resampled, y_train_resampled, tokenizer, max_len)\n",
    "    test_dataset = CareerDataset(X_test, y_test, tokenizer, max_len)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Load pretrained BERT model for classification\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        'bert-base-uncased', num_labels=len(y.unique()))\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "        print(f\"Epoch {epoch+1} completed. Avg Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    # Evaluation on test set\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            _, predicted = torch.max(logits, dim=1)\n",
    "\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    print(\"\\nOversampling Results:\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Counter({0: 10200, 1: 8310, 2: 6763})\n",
      "Training : Counter({0: 7140, 1: 5817, 2: 4734})\n",
      "Test : Counter({0: 3060, 1: 2493, 2: 2029})\n",
      "BEFORE oversampling: Counter({0: 7140, 1: 5817, 2: 4734})\n",
      "AFTER oversampling: Counter({2: 7140, 0: 7140, 1: 7140})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed. Avg Loss: 0.5528\n",
      "Epoch 2 completed. Avg Loss: 0.3396\n",
      "Epoch 3 completed. Avg Loss: 0.3256\n",
      "Epoch 4 completed. Avg Loss: 0.3233\n",
      "Epoch 5 completed. Avg Loss: 0.3230\n",
      "\n",
      "Oversampling Results:\n",
      "Accuracy: 0.861514112371406\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.81      0.88      3060\n",
      "           1       0.81      0.94      0.87      2493\n",
      "           2       0.79      0.85      0.82      2029\n",
      "\n",
      "    accuracy                           0.86      7582\n",
      "   macro avg       0.86      0.87      0.86      7582\n",
      "weighted avg       0.87      0.86      0.86      7582\n",
      "\n",
      "[[2467  261  332]\n",
      " [  42 2336  115]\n",
      " [  25  275 1729]]\n",
      "✅ Model and tokenizer saved.\n"
     ]
    }
   ],
   "source": [
    "# Oversampling with 70/30 split\n",
    "model, tokenizer =run_bert_oversampling(X, y, train_size=0.7, test_size=0.3, tokenizer=tokenizer)\n",
    "\n",
    "# Save\n",
    "model.save_pretrained(\"bert_finetuned_over_70\")\n",
    "tokenizer.save_pretrained(\"bert_finetuned_over_70\")\n",
    "print(\"✅ Model and tokenizer saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Counter({0: 10200, 1: 8310, 2: 6763})\n",
      "Training : Counter({0: 8160, 1: 6648, 2: 5410})\n",
      "Test : Counter({0: 2040, 1: 1662, 2: 1353})\n",
      "BEFORE oversampling: Counter({0: 8160, 1: 6648, 2: 5410})\n",
      "AFTER oversampling: Counter({1: 8160, 0: 8160, 2: 8160})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed. Avg Loss: 0.5112\n",
      "Epoch 2 completed. Avg Loss: 0.3350\n",
      "Epoch 3 completed. Avg Loss: 0.3221\n",
      "Epoch 4 completed. Avg Loss: 0.3188\n",
      "Epoch 5 completed. Avg Loss: 0.3176\n",
      "\n",
      "Oversampling Results:\n",
      "Accuracy: 0.858160237388724\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.79      0.87      2040\n",
      "           1       0.81      0.94      0.87      1662\n",
      "           2       0.79      0.86      0.82      1353\n",
      "\n",
      "    accuracy                           0.86      5055\n",
      "   macro avg       0.86      0.86      0.86      5055\n",
      "weighted avg       0.87      0.86      0.86      5055\n",
      "\n",
      "[[1612  193  235]\n",
      " [  21 1562   79]\n",
      " [  15  174 1164]]\n",
      "✅ Model and tokenizer saved.\n"
     ]
    }
   ],
   "source": [
    "#Oversampling with 80/20 split\n",
    "model, tokenizer =run_bert_oversampling(X, y, train_size=0.8, test_size=0.2, tokenizer=tokenizer)\n",
    "\n",
    "# Save\n",
    "model.save_pretrained(\"bert_finetuned_over_80_20\")\n",
    "tokenizer.save_pretrained(\"bert_finetuned_over_80_20\")\n",
    "print(\"✅ Model and tokenizer saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Counter({0: 10200, 1: 8310, 2: 6763})\n",
      "Training : Counter({0: 9180, 1: 7479, 2: 6086})\n",
      "Test : Counter({0: 1020, 1: 831, 2: 677})\n",
      "BEFORE oversampling: Counter({0: 9180, 1: 7479, 2: 6086})\n",
      "AFTER oversampling: Counter({1: 9180, 0: 9180, 2: 9180})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed. Avg Loss: 0.5198\n",
      "Epoch 2 completed. Avg Loss: 0.3289\n",
      "Epoch 3 completed. Avg Loss: 0.3161\n",
      "Epoch 4 completed. Avg Loss: 0.3132\n",
      "Epoch 5 completed. Avg Loss: 0.3135\n",
      "\n",
      "Oversampling Results:\n",
      "Accuracy: 0.8635284810126582\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.81      0.89      1020\n",
      "           1       0.81      0.94      0.87       831\n",
      "           2       0.80      0.84      0.82       677\n",
      "\n",
      "    accuracy                           0.86      2528\n",
      "   macro avg       0.86      0.87      0.86      2528\n",
      "weighted avg       0.88      0.86      0.86      2528\n",
      "\n",
      "[[827  90 103]\n",
      " [  8 784  39]\n",
      " [  7  98 572]]\n",
      "✅ Model and tokenizer saved.\n"
     ]
    }
   ],
   "source": [
    "# Oversampling with 90/10 split\n",
    "model, tokenizer =run_bert_oversampling(X, y, train_size=0.9, test_size=0.1, tokenizer=tokenizer)\n",
    "\n",
    "# Save\n",
    "model.save_pretrained(\"bert_finetuned_over_90_10\")\n",
    "tokenizer.save_pretrained(\"bert_finetuned_over_90_10\")\n",
    "print(\"✅ Model and tokenizer saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. UNDERSAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "def run_bert_undersampling(X, y, train_size, test_size, tokenizer, max_len=128, batch_size=16, epochs=5):\n",
    "    # Split with stratify to maintain class distribution in splits\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, train_size=train_size, test_size=test_size, random_state=42, stratify=y)\n",
    "\n",
    "    # Undersample training data\n",
    "    rus = RandomUnderSampler(random_state=42)\n",
    "    X_train_array = X_train.values.reshape(-1, 1)\n",
    "    X_train_resampled, y_train_resampled = rus.fit_resample(X_train_array, y_train)\n",
    "\n",
    "    # Convert back to pandas Series for your Dataset\n",
    "    X_train_resampled = pd.Series(X_train_resampled.flatten())\n",
    "    y_train_resampled = pd.Series(y_train_resampled)\n",
    "\n",
    "    # Create Dataset and DataLoader for training and testing\n",
    "    train_dataset = CareerDataset(X_train_resampled, y_train_resampled, tokenizer, max_len)\n",
    "    test_dataset = CareerDataset(X_test, y_test, tokenizer, max_len)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Load pretrained BERT model for classification\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        'bert-base-uncased', num_labels=len(y.unique()))\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "        print(f\"Epoch {epoch+1} completed. Avg Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    # Evaluation on test set\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            _, predicted = torch.max(logits, dim=1)\n",
    "\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    print(\"\\nUndersampling Results:\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    " \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed. Avg Loss: 0.6982\n",
      "Epoch 2 completed. Avg Loss: 0.3583\n",
      "Epoch 3 completed. Avg Loss: 0.3483\n",
      "Epoch 4 completed. Avg Loss: 0.3478\n",
      "Epoch 5 completed. Avg Loss: 0.3480\n",
      "\n",
      "Undersampling Results:\n",
      "Accuracy: 0.8579530466895279\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.79      0.88      3060\n",
      "           1       0.80      0.96      0.87      2493\n",
      "           2       0.79      0.85      0.82      2029\n",
      "\n",
      "    accuracy                           0.86      7582\n",
      "   macro avg       0.86      0.86      0.85      7582\n",
      "weighted avg       0.87      0.86      0.86      7582\n",
      "\n",
      "[[2404  308  348]\n",
      " [   5 2386  102]\n",
      " [   7  307 1715]]\n",
      "✅ Model and tokenizer saved.\n"
     ]
    }
   ],
   "source": [
    "# Undersampling with 70/30 split\n",
    "model, tokenizer =run_bert_undersampling(X, y, train_size=0.7, test_size=0.3, tokenizer=tokenizer)\n",
    "\n",
    "# Save\n",
    "model.save_pretrained(\"bert_finetuned_under_70\")\n",
    "tokenizer.save_pretrained(\"bert_finetuned_under_70\")\n",
    "print(\"✅ Model and tokenizer saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed. Avg Loss: 0.6042\n",
      "Epoch 2 completed. Avg Loss: 0.3574\n",
      "Epoch 3 completed. Avg Loss: 0.3483\n",
      "Epoch 4 completed. Avg Loss: 0.3458\n",
      "Epoch 5 completed. Avg Loss: 0.3452\n",
      "\n",
      "Undersampling Results:\n",
      "Accuracy: 0.8573689416419387\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.79      0.87      2040\n",
      "           1       0.81      0.94      0.87      1662\n",
      "           2       0.79      0.85      0.82      1353\n",
      "\n",
      "    accuracy                           0.86      5055\n",
      "   macro avg       0.86      0.86      0.85      5055\n",
      "weighted avg       0.87      0.86      0.86      5055\n",
      "\n",
      "[[1613  193  234]\n",
      " [  26 1569   67]\n",
      " [  14  187 1152]]\n",
      "✅ Model and tokenizer saved.\n"
     ]
    }
   ],
   "source": [
    "# Undersampling with 80/20 split\n",
    "model, tokenizer =run_bert_undersampling(X, y, train_size=0.8, test_size=0.2, tokenizer=tokenizer)\n",
    "\n",
    "# Save\n",
    "model.save_pretrained(\"bert_finetuned_under_80\")\n",
    "tokenizer.save_pretrained(\"bert_finetuned_under_80\")\n",
    "print(\"✅ Model and tokenizer saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed. Avg Loss: 0.5768\n",
      "Epoch 2 completed. Avg Loss: 0.3535\n",
      "Epoch 3 completed. Avg Loss: 0.3433\n",
      "Epoch 4 completed. Avg Loss: 0.3395\n",
      "Epoch 5 completed. Avg Loss: 0.3396\n",
      "\n",
      "Undersampling Results:\n",
      "Accuracy: 0.8643196202531646\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.80      0.89      1020\n",
      "           1       0.81      0.95      0.87       831\n",
      "           2       0.80      0.85      0.82       677\n",
      "\n",
      "    accuracy                           0.86      2528\n",
      "   macro avg       0.87      0.87      0.86      2528\n",
      "weighted avg       0.88      0.86      0.87      2528\n",
      "\n",
      "[[817  91 112]\n",
      " [  7 791  33]\n",
      " [  1  99 577]]\n",
      "✅ Model and tokenizer saved.\n"
     ]
    }
   ],
   "source": [
    "# Undersampling with 90/10 split\n",
    "model, tokenizer =run_bert_undersampling(X, y, train_size=0.9, test_size=0.1, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "# Save\n",
    "model.save_pretrained(\"bert_finetuned_under_90\")\n",
    "tokenizer.save_pretrained(\"bert_finetuned_under_90\")\n",
    "print(\"✅ Model and tokenizer saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
